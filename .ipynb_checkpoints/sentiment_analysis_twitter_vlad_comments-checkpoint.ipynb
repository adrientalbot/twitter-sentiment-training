{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/creating-the-twitter-sentiment-analysis-program-in-python-with-naive-bayes-classification-672e5589a7ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticating Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Tue Jun 16 20:29:26 +0000 2020\", \"default_profile\": true, \"default_profile_image\": true, \"id\": 1272989631404015616, \"id_str\": \"1272989631404015616\", \"name\": \"Nicola Osrin\", \"profile_background_color\": \"F5F8FA\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"NicolaOsrin\"}\n"
     ]
    }
   ],
   "source": [
    "# Authenticating our twitter API credentials\n",
    "twitter_api = twitter.Api(consumer_key='f2ujCRaUnQJy4PoiZvhRQL4n4',\n",
    "                        consumer_secret='EjBSQirf7i83T7CX90D5Qxgs9pTdpIGIsVAhHVs5uvd0iAcw5V',\n",
    "                        access_token_key='1272989631404015616-5XMQkx65rKfQU87UWAh40cMf4aCzSq',\n",
    "                        access_token_secret='emfWcF8fyfqoyywfPCJnz4jXt6DFXfndro59UK9IMAMgy')\n",
    "\n",
    "# Test authentication to make sure it was successful\n",
    "print(twitter_api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first build the test set, consisting of only 100 tweets for simplicity. \n",
    "#Note that we can only download 180 tweets every 15min.\n",
    "def buildTestSet(search_keyword):\n",
    "    try:\n",
    "        tweets_fetched = twitter_api.GetSearch(search_keyword, count = 100)\n",
    "        \n",
    "        print(\"Fetched \" + str(len(tweets_fetched)) + \" tweets for the term \" + search_keyword)\n",
    "        \n",
    "        return [{\"text\":status.text, \"label\":None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Unfortunately, something went wrong..\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search keyword:stonks\n",
      "Fetched 100 tweets for the term stonks\n",
      "[{'text': 'STONKS https://t.co/lqgfsr29YD', 'label': None}, {'text': 'giveaway yey, honestly the best\\nrules:\\n\\nfollow\\nretweet\\ncomment\\nlike\\nwin?\\nstonks\\n\\nwinner sunday at some point in dm… https://t.co/d7dDHIsGpl', 'label': None}, {'text': 'Okay, I finally did it! I deposited the $10 my mom gave me in a Robinhood account and bought some stonks. I’m alrea… https://t.co/7HUftQOoEQ', 'label': None}, {'text': '@Alexandreeabreu Eu só perco na canastra pra mãe, stonks', 'label': None}]\n"
     ]
    }
   ],
   "source": [
    "#Testing out fetching the test set. The function below prints out the first 5 tweets in our test set.\n",
    "search_term = input(\"Enter a search keyword:\")\n",
    "testDataSet = buildTestSet(search_term)\n",
    "\n",
    "print(testDataSet[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(list())\n",
    "#df.to_csv('tweetDataFile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a downloadable training set, consisting of 5,000 tweets. These tweets have already been labelled as positive/negative. We use this training set to calculate the posterior probabilities of each word appearing and its respective sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As Twitter doesn't allow the storage of the tweets on personal drives, we have to create a function to download\n",
    "#the relevant tweets that will be matched to the Tweet IDs and their labels, which we have.\n",
    "\n",
    "def buildTrainingSet(corpusFile, tweetDataFile, size):\n",
    "    import csv\n",
    "    import time\n",
    "    \n",
    "    count = 0\n",
    "    corpus = []\n",
    "    \n",
    "    with open(corpusFile,'r') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',', quotechar=\"\\\"\")\n",
    "        for row in lineReader:\n",
    "            if count <= size: \n",
    "                corpus.append({\"tweet_id\":row[2], \"label\":row[1], \"topic\":row[0]})\n",
    "                count += 1\n",
    "            else: \n",
    "                break\n",
    "\n",
    "    rate_limit = 180\n",
    "    sleep_time = 900/180\n",
    "    \n",
    "    trainingDataSet = []\n",
    "    \n",
    "    for tweet in corpus:\n",
    "        try:\n",
    "            status = twitter_api.GetStatus(tweet[\"tweet_id\"])\n",
    "            print(\"Tweet fetched\" + status.text)\n",
    "            tweet[\"text\"] = status.text\n",
    "            trainingDataSet.append(tweet)\n",
    "            time.sleep(sleep_time) \n",
    "        except: \n",
    "            continue\n",
    "    # now we write them to the empty CSV file\n",
    "    with open(tweetDataFile,'w') as csvfile:\n",
    "        linewriter = csv.writer(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for tweet in trainingDataSet:\n",
    "            try:\n",
    "                linewriter.writerow([tweet[\"tweet_id\"], tweet[\"text\"], tweet[\"label\"], tweet[\"topic\"]])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return trainingDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorpusFile = \"corpus.csv\"\\ntweetDataFile = \"tweetDataFile.csv\"\\n\\ntrainingData = buildTrainingSet(corpusFile, tweetDataFile, 1000)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function is used to download the actual tweets. It takes hours to run and we only need to run it once\n",
    "#in order to get all 5,000 training tweets. The 'size' parameter below is the number of tweets that we want to\n",
    "#download. If 5,000 => set size=5,000\n",
    "'''\n",
    "corpusFile = \"corpus.csv\"\n",
    "tweetDataFile = \"tweetDataFile.csv\"\n",
    "\n",
    "trainingData = buildTrainingSet(corpusFile, tweetDataFile, 1000)\n",
    "'''\n",
    "\n",
    "#When this code stops running, we will have a tweetDataFile.csv full of the tweets that we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       2503\n",
       "irrelevant    1786\n",
       "negative       654\n",
       "positive       569\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line counts the number of tweets and their labels in the Corpus.csv file that we originally downloaded\n",
    "corp = pd.read_csv(\"corpus.csv\", header = 0 , names = ['topic','label', 'tweet_id'] )\n",
    "corp['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126415614616154112</td>\n",
       "      <td>Now all @Apple has to do is get swype on the i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126402758403305474</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet wit...</td>\n",
       "      <td>positive</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126397179614068736</td>\n",
       "      <td>@RIM you made it too easy for me to switch to ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126379685453119488</td>\n",
       "      <td>The 16 strangest things Siri has said so far. ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126377656416612353</td>\n",
       "      <td>Great up close &amp; personal event @Apple tonight...</td>\n",
       "      <td>positive</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text  \\\n",
       "0  126415614616154112  Now all @Apple has to do is get swype on the i...   \n",
       "1  126402758403305474  Hilarious @youtube video - guy does a duet wit...   \n",
       "2  126397179614068736  @RIM you made it too easy for me to switch to ...   \n",
       "3  126379685453119488  The 16 strangest things Siri has said so far. ...   \n",
       "4  126377656416612353  Great up close & personal event @Apple tonight...   \n",
       "\n",
       "      label  topic  \n",
       "0  positive  apple  \n",
       "1  positive  apple  \n",
       "2  positive  apple  \n",
       "3  positive  apple  \n",
       "4  positive  apple  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As a check, we look at the first 5 lines in our new tweetDataFile.csv\n",
    "trainingData_copied = pd.read_csv(\"tweetDataFile.csv\", header = None, names = ['tweet_id', 'text', 'label', 'topic'])\n",
    "trainingData_copied.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     316\n",
       "negative    250\n",
       "positive    114\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We check the number of tweets by each label in our training set\n",
    "trainingData_copied['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_copied = trainingData_copied.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the NLTK library to filter for keywords and remove irrelevant words in tweets. We also remove punctuation and things like images (emojis) as they cannot be classified using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #a library that makes parsing strings and modifying them more efficient\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "import nltk #Natural Processing Toolkit that takes care of any processing that we need to perform on text \n",
    "            #to change its form or extract certain components from it.\n",
    "    \n",
    "#nltk.download('popular') #We need this if certain nltk libraries are not installed. \n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        tweet = tweet.lower() # convert text to lower-case\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "        tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "        tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)\n",
    "        return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we call the function to pre-process both our training and our test set. \n",
    "tweetProcessor = PreProcessTweets()\n",
    "preprocessedTrainingSet = tweetProcessor.processTweets(trainingData_copied)\n",
    "preprocessedTestSet = tweetProcessor.processTweets(testDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a classifier based on Bayes' Theorem, hence the name. It allows us to find the posterior probability of an event occuring (in this case that event being the sentiment- positive/neutral or negative) is reliant on another probabilistic background that we know. \n",
    "\n",
    "The posterior probability is calculated as follows:\n",
    "$P(A|B) = \\frac{P(B|A)\\times P(A)}{P(B)}$\n",
    "\n",
    "The final sentiment is assigned based on the highest probability of the tweet falling in each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we attempt to build a vocabulary (a list of words) of all words present in the training set.\n",
    "\n",
    "import nltk \n",
    "\n",
    "def buildVocabulary(preprocessedTrainingData):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in preprocessedTrainingData:\n",
    "        all_words.extend(words)\n",
    "\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features\n",
    "\n",
    "#This function generates a list of all words (all_words) and then turns it into a frequency distribution (wordlist)\n",
    "#The word_features is a list of distinct words, with the key being the frequency of each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching tweets against our vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go through all the words in the training set (i.e. our word_features list), comparing every word against the tweet at hand, associating a number with the word following:\n",
    "\n",
    "label 1 (true): if word in vocabulary occurs in tweet\n",
    "\n",
    "label 0 (false): if word in vocabulary does not occur in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = buildVocabulary(preprocessedTrainingSet)\n",
    "trainingFeatures = nltk.classify.apply_features(extract_features, preprocessedTrainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature vector shows if a particular tweet contains a certain word out of all the words present in the corpus in the training data + label (positive, negative or neutral) of the tweet.\n",
    "\n",
    "We will feed this in the Naive Bayes Classifier, which will calculate the posterior probability given the prior probability of having a specific word in the tweet, and the likelihood of the specified outcome given the presence of this word based on the trainingFeatures data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Naives Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line trains our Bayes Classifier\n",
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Negative Sentiment\n",
      "Negative Sentiment Percentage = 4.0%\n",
      "Positive Sentiment Percentage = 1.0%\n",
      "Number of negative comments = 4\n",
      "Number of positive comments = 1\n",
      "Number of neutral comments = 95\n"
     ]
    }
   ],
   "source": [
    "#We now run the classifier and test it on 100 tweets previously downloaded in the test set, on our specified keyword.\n",
    "\n",
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in preprocessedTestSet]\n",
    "\n",
    "# get the majority vote\n",
    "if NBResultLabels.count('positive') > NBResultLabels.count('negative'):\n",
    "    print(\"Overall Positive Sentiment\")\n",
    "    print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels.count('positive')/len(NBResultLabels)) + \"%\")\n",
    "else: \n",
    "    print(\"Overall Negative Sentiment\")\n",
    "    print(\"Negative Sentiment Percentage = \" + str(100*NBResultLabels.count('negative')/len(NBResultLabels)) + \"%\")\n",
    "    print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels.count('positive')/len(NBResultLabels)) + \"%\")\n",
    "    print(\"Number of negative comments = \" + str(NBResultLabels.count('negative')))\n",
    "    print(\"Number of positive comments = \" + str(NBResultLabels.count('positive')))\n",
    "    print(\"Number of neutral comments = \" + str(NBResultLabels.count('neutral')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
